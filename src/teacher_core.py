# -*- coding: utf-8 -*-
"""teacher_core.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11kQ6aFsJEpPNH_6QLnvn2aiGVkwd98bW
"""

import os
import torch
import torch.nn.functional as F
from utils import count_params, measure_latency, evaluate_accuracy
from model_helpers import setup_models, extract_teacher_features

print(f"PyTorch Version: {torch.__version__}")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

def train_teacher(teacher, loader, val_loader, epochs, tag, lr=1e-3, save_path="Teachermodel.pth"):
    """
    Trains a model with Adam and cross-entropy loss.
    Loads from save_path if it exists.
    """

    if os.path.exists(save_path):
        print(f"Model already trained. Loading from {save_path}")
        teacher.load_state_dict(torch.load(save_path))
        return teacher

    # no saved model found. training from given model state

    optimizer = torch.optim.Adam(teacher.parameters(), lr=1e-3)
    teacher.train()

    for epoch in range(epochs):
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            logits, _ = extract_teacher_features(teacher, inputs)

            loss = F.cross_entropy(logits, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        accuracy = evaluate_accuracy(teacher, val_loader)
        print(f"({tag})\tEpoch {epoch+1}: loss={loss.item():.4f}, Accuracy (validation): {accuracy*100:.2f}%")
        teacher.train()

    if save_path:
        torch.save(teacher.state_dict(), save_path)
        print(f"Training complete. Model saved to {save_path}")

    return teacher