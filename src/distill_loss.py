# -*- coding: utf-8 -*-
"""distill_loss.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fzq1qmrwRkIKolF_EEkL4Tw4mS31BcQI

Added Temperature Scheduling and alpha scheduling instead of using fixed values as hyperparameters
"""

import torch
import torch.nn.functional as F
from model_helpers import setup_models, extract_teacher_features

print(f"PyTorch Version: {torch.__version__}")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

def exp_decay_with_floor(start, min_val, gamma, epoch):
    """
    Exponential decay with a floor value.
    
    Args:
        start (float): Initial value at epoch 0.
        min_val (float): Minimum value the parameter can decay to.
        gamma (float): Decay rate per epoch (0 < gamma < 1).
        epoch (int): Current epoch number.
    
    Returns:
        float: Decayed value with floor.
    """
    value = start * (gamma ** epoch)
    return max(min_val, value)

# Define exp_decay for scheduling
def exp_decay(start, gamma, epoch):
    return start * (gamma ** epoch)


def distillation_loss(student_logits, teacher_logits, targets, T, alpha):
    """
    Combine soft and hard targets using KL divergence and cross-entropy
    T = temperature, alpha = weighting between soft and hard losses
    """

    # soft target loss (teacher softmax vs student softmax)
    soft_targets = F.kl_div(
        F.log_softmax(student_logits / T, dim=1),
        F.softmax(teacher_logits / T, dim=1),
        reduction='batchmean'
    ) * (T * T)

    # hard label loss
    hard_loss = F.cross_entropy(student_logits, targets)
    return alpha * soft_targets + (1 - alpha) * hard_loss

def student_training_step(inputs, labels, teacher, student_wrapper, optimizer, epoch, T, alpha, device):
    """
    Perform a single training step for the student model using knowledge distillation.
    """

    inputs, labels = inputs.to(device), labels.to(device)

    # Schedule temperature: start at 5.0 and decay gradually to 1.0
    #T = max(1.0, 5.0 - 0.1 * epoch)
    #T = max(1.0, exp_decay(5.0, 0.92, epoch))
    T = exp_decay_with_floor(5.0, 3.0, 0.95, epoch)    # T won't go below 2.0
    # Schedule alpha: start high (focus on soft targets), decrease to focus more on hard labels
    #alpha = max(0.1, 0.7 - 0.01 * epoch)
    #alpha = max(0.1, exp_decay(0.7, 0.92, epoch))
    alpha = exp_decay_with_floor(0.8, 0.5, 0.95, epoch) # alpha won't go below 0.3
    # extract teacher logits and intermediate features
    with torch.no_grad():
        teacher_logits, teacher_feats = extract_teacher_features(teacher, inputs)

    # extract student logits and intermediate features
    student_logits, student_feats = student_wrapper(inputs)
    projected_feats = student_wrapper.project_features(student_feats, [t.shape for t in teacher_feats])

    # calculate loss from features difference
    feat_loss = sum(F.mse_loss(p, t.detach()) for p, t in zip(projected_feats, teacher_feats))

    # calculate loss from output distribution, and include feature loss
    loss = distillation_loss(student_logits, teacher_logits, labels, T, alpha) + 0.1 * feat_loss

    # optimize with loss
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()