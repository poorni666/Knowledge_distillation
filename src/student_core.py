# -*- coding: utf-8 -*-
"""student_core.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sKwfMHBfG0YZFi97Fa6cGvxAOkw3IpYh
"""

import os
import torch
import torch.nn.functional as F
from distill_loss import distillation_loss, student_training_step
from utils import count_params, measure_latency, evaluate_accuracy
from model_helpers import setup_models, extract_teacher_features
from torch.optim.lr_scheduler import ReduceLROnPlateau

print(f"PyTorch Version: {torch.__version__}")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

def exp_decay_with_floor(start, min_val, gamma, epoch):
    """
    Exponential decay with a floor value.
    
    Args:
        start (float): Initial value at epoch 0.
        min_val (float): Minimum value the parameter can decay to.
        gamma (float): Decay rate per epoch (0 < gamma < 1).
        epoch (int): Current epoch number.
    
    Returns:
        float: Decayed value with floor.
    """
    value = start * (gamma ** epoch)
    return max(min_val, value)

# Define exp_decay for scheduling 
def exp_decay(start, gamma, epoch):
    return start * (gamma ** epoch)
    
def train_student(teacher, student_wrapper, dataloader, val_loader, epochs, device, fixed_T =None, fixed_alpha=None,save_path="student_distilled.pth"):
    """
    Trains a student model using knowledge distillation from a teacher model.
    """

    # setup optimizer
    optimizer = torch.optim.Adam(student_wrapper.parameters(), lr=1e-3)

    # train the student using the teacher's output as soft targets
    teacher.eval()

    best_val_acc = 0.0

    # reduce LR if validation loss doesn't improve for 3 epochs
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

    for epoch in range(epochs):
        student_wrapper.train()
        running_loss = 0
        for inputs, labels in dataloader:
            if fixed_T is not None and fixed_alpha is not None:
                T = fixed_T
                alpha = fixed_alpha
            else:
                #T = max(1.0, 5.0 - 0.1 * epoch)
                #alpha = max(0.1, 0.7 - 0.01 * epoch)
                #T = max(1.0, exp_decay(5.0, 0.92, epoch))
                #alpha = max(0.1, exp_decay(0.7, 0.92, epoch))
                T = exp_decay_with_floor(5.0, 3.0, 0.95, epoch)    # T won't go below 2.0
                alpha = exp_decay_with_floor(0.8, 0.5, 0.95, epoch) # alpha won't go below 0.3

            loss = student_training_step(inputs, labels, teacher, student_wrapper, optimizer, epoch, T, alpha, device)
            running_loss += loss

        val_acc = evaluate_accuracy(student_wrapper.model, val_loader)
        print(f"[(Training student)\tEpoch {epoch+1}] Loss = {running_loss/len(dataloader):.4f} | Val Acc = {val_acc*100:.2f}%")
        scheduler.step(loss)

        # save best checkpoint
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(student_wrapper.state_dict(), save_path)
            print("New best model saved.")

    # load best checkpoint
    student_wrapper.load_state_dict(torch.load(save_path))
    student = student_wrapper.model

    return student