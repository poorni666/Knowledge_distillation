# -*- coding: utf-8 -*-
"""model_helpers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r9YWK7VvJnJ16j9UU6KMwBamaNDGylye
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

print(f"PyTorch Version: {torch.__version__}")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

def setup_models(device):
    """
    Setup teacher and student wrapper
    """

    # teacher: ResNet50 pretrained on ImageNet, re-headed for CIFAR-10
    teacher = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
    teacher.fc = nn.Linear(2048, 10)
    teacher = teacher.to(device)

    # student: ResNet18 without pretrained weights
    student = models.resnet18(weights=None)
    student.fc = nn.Linear(512, 10)
    student = student.to(device)

    # define the intermediate feature channels for both teacher and student
    student_channels = [64, 128, 256, 512]
    teacher_channels = [256, 512, 1024, 2048]

    # create projection layers to align teacher's feature maps with student's feature maps
    proj_layers = [
        FeatureProjector(in_c, out_c).to(device)
        for in_c, out_c in zip(student_channels, teacher_channels)
    ]

    # wrap the student model with the projection layers
    student_wrapper = StudentWrapper(student, proj_layers).to(device)

    return teacher, student_wrapper

class FeatureProjector(nn.Module):
    """
    Feature projector to match student -> teacher feature shapes
    """

    def __init__(self, in_channels, out_channels):
        super().__init__()

        # define a 1x1 convolutional layer to project feature maps
        self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x, target_shape):

        # check if the spatial dimensions of the input match the target shape
        if x.shape[2:] != target_shape[2:]:

            # adjust spatial dimensions using adaptive average pooling
            x = F.adaptive_avg_pool2d(x, output_size=target_shape[2:])

        # apply the projection layer to transform feature maps
        return self.proj(x)

class StudentWrapper(nn.Module):
    """
    Wrapper for the student model with projection layers
    """

    def __init__(self, student_model, proj_layers):
        super().__init__()

        # store student model
        self.model = student_model

        # store projection layers for feature alignment
        self.projections = nn.ModuleList(proj_layers)

    def forward(self, x):

        # collect intermediate features from ResNet blocks
        features = []
        x = self.model.conv1(x)
        x = self.model.bn1(x)
        x = self.model.relu(x)
        x = self.model.maxpool(x)
        for i, block in enumerate([self.model.layer1, self.model.layer2, self.model.layer3, self.model.layer4]):
            # pass through ResNet blocks
            x = block(x)

            # append features from each block
            features.append(x)

        # pool the final feature map and compute logits
        pooled = F.adaptive_avg_pool2d(x, (1, 1))
        flat = torch.flatten(pooled, 1)
        logits = self.model.fc(flat)

        return logits, features

    def project_features(self, features, target_shapes):
        """
        Project student features to match the shapes of teacher features.
        """

        return [
            proj(s_feat, t_shape)
            for s_feat, t_shape, proj in zip(features, target_shapes, self.projections)
        ]

def extract_teacher_features(model, x, layers=[1, 2, 3, 4]):
    """
    Extract teacher logits and intermediate features
    """

    # collect intermediate features from ResNet blocks
    features = []
    x = model.conv1(x)
    x = model.bn1(x)
    x = model.relu(x)
    x = model.maxpool(x)
    for i, block in enumerate([model.layer1, model.layer2, model.layer3, model.layer4]):
        x = block(x)
        if (i + 1) in layers:
            features.append(x)

    # pool the final feature map and compute logits
    pooled = F.adaptive_avg_pool2d(x, (1, 1))  # [B, C, 1, 1]
    flat = torch.flatten(pooled, 1)            # [B, C]
    logits = model.fc(flat)                    # [B, 10]
    return logits, features